# ===================================================================
# ALGORITMO GEN√âTICO PARA OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS
# ===================================================================
# 
# OBJETIVO: Encontrar la combinaci√≥n √≥ptima de hiperpar√°metros que
# maximiza el rendimiento de un modelo de machine learning.
#
# PROBLEMA: Los modelos de ML tienen m√∫ltiples hiperpar√°metros que
# afectan su rendimiento. ¬øC√≥mo encontrar los valores √≥ptimos?
#
# SOLUCI√ìN: Usar un Algoritmo Gen√©tico para explorar el espacio de
# hiperpar√°metros y encontrar la mejor combinaci√≥n.
# ===================================================================

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, classification_report
from sklearn.exceptions import ConvergenceWarning
import warnings
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# ===================================================================
# 1. CONFIGURACI√ìN DEL ALGORITMO GEN√âTICO
# ===================================================================
# Par√°metros que controlan el comportamiento del algoritmo gen√©tico

RANDOM_STATE = 42           # Semilla para reproducibilidad
POBLACION_INICIAL = 20      # Tama√±o de la poblaci√≥n (menor que feature selection)
GENERACIONES = 15           # N√∫mero de generaciones (menor que feature selection)
PROB_CRUCE = 0.8           # Probabilidad de cruce entre padres
PROB_MUTACION = 0.1        # Probabilidad de mutaci√≥n (mayor que feature selection)
TORNEO_K = 3               # Tama√±o del torneo para selecci√≥n
KFOLD = 5                  # N√∫mero de folds para validaci√≥n cruzada

# Establecer semilla aleatoria
np.random.seed(RANDOM_STATE)

print("=" * 60)
print("ALGORITMO GEN√âTICO PARA OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS")
print("=" * 60)
print(f"Poblaci√≥n: {POBLACION_INICIAL} individuos")
print(f"Generaciones: {GENERACIONES}")
print(f"Probabilidad de cruce: {PROB_CRUCE}")
print(f"Probabilidad de mutaci√≥n: {PROB_MUTACION}")
print(f"Validaci√≥n cruzada: {KFOLD}-fold")
print("=" * 60)

# ===================================================================
# 2. CARGA Y PREPARACI√ìN DE DATOS
# ===================================================================
# Cargamos el dataset y lo preparamos para la optimizaci√≥n

print("\nüìä CARGANDO DATOS...")
df = pd.read_csv("dataset_1_convertido.csv")
df.columns = df.columns.str.strip()

# Separar caracter√≠sticas y variable objetivo
if "stroke" not in df.columns:
    raise ValueError("No se encontr√≥ la columna objetivo 'stroke' en el dataset.")

y = df["stroke"].astype(int)

# Excluir columnas no predictoras
cols_excluir = [c for c in ["stroke", "id"] if c in df.columns]
X = df.drop(columns=cols_excluir)

print(f"‚úÖ Dataset cargado: {X.shape[0]} muestras, {X.shape[1]} caracter√≠sticas")
print(f"‚úÖ Variable objetivo: {y.value_counts().to_dict()}")

# ===================================================================
# 3. DEFINICI√ìN DEL ESPACIO DE HIPERPAR√ÅMETROS
# ===================================================================
# Definimos los hiperpar√°metros a optimizar y sus rangos de valores

# Configuraci√≥n de hiperpar√°metros para diferentes algoritmos
HIPERPARAMETROS = {
    'LogisticRegression': {
        'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],  # Regularizaci√≥n inversa
        'penalty': ['l1', 'l2'],                              # Tipo de regularizaci√≥n
        'solver': ['liblinear', 'saga'],                      # Algoritmo de optimizaci√≥n
        'max_iter': [100, 200, 500, 1000]                    # M√°ximo de iteraciones
    },
    'RandomForestClassifier': {
        'n_estimators': [50, 100, 200, 300, 500],           # N√∫mero de √°rboles
        'max_depth': [3, 5, 10, 15, 20, None],              # Profundidad m√°xima
        'min_samples_split': [2, 5, 10, 20],                # M√≠nimo muestras para dividir
        'min_samples_leaf': [1, 2, 4, 8],                   # M√≠nimo muestras por hoja
        'max_features': ['sqrt', 'log2', 0.5, 0.8]          # Caracter√≠sticas por divisi√≥n
    },
    'SVC': {
        'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],         # Par√°metro de regularizaci√≥n
        'kernel': ['linear', 'rbf', 'poly'],                # Tipo de kernel
        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1.0], # Coeficiente del kernel
        'degree': [2, 3, 4, 5]                              # Grado del polinomio (solo para poly)
    }
}

# Seleccionar el algoritmo a optimizar (cambiar aqu√≠ para probar otros)
ALGORITMO_SELECCIONADO = 'LogisticRegression'  # Cambiar a 'RandomForestClassifier' o 'SVC'

print(f"\nüéØ Algoritmo seleccionado para optimizaci√≥n: {ALGORITMO_SELECCIONADO}")
print(f"üìã Hiperpar√°metros a optimizar: {list(HIPERPARAMETROS[ALGORITMO_SELECCIONADO].keys())}")

# ===================================================================
# 4. FUNCIONES DE CODIFICACI√ìN Y DECODIFICACI√ìN
# ===================================================================
# Estas funciones convierten entre hiperpar√°metros y representaci√≥n gen√©tica

def codificar_hiperparametros(hiperparametros_dict):
    """
    Convierte un diccionario de hiperpar√°metros a un vector num√©rico.
    
    Args:
        hiperparametros_dict: Diccionario con hiperpar√°metros
    
    Returns:
        numpy.array: Vector codificado
    """
    vector = []
    
    for param_name, param_value in hiperparametros_dict.items():
        if param_name in HIPERPARAMETROS[ALGORITMO_SELECCIONADO]:
            # Encontrar el √≠ndice del valor en la lista de opciones
            opciones = HIPERPARAMETROS[ALGORITMO_SELECCIONADO][param_name]
            if param_value in opciones:
                idx = opciones.index(param_value)
                vector.append(idx)
            else:
                # Si el valor no est√° en las opciones, usar valor aleatorio
                vector.append(np.random.randint(0, len(opciones)))
        else:
            # Si el par√°metro no est√° en la configuraci√≥n, usar 0
            vector.append(0)
    
    return np.array(vector)

def decodificar_hiperparametros(vector_codificado):
    """
    Convierte un vector num√©rico a un diccionario de hiperpar√°metros.
    
    Args:
        vector_codificado: Vector codificado
    
    Returns:
        dict: Diccionario con hiperpar√°metros
    """
    hiperparametros = {}
    param_names = list(HIPERPARAMETROS[ALGORITMO_SELECCIONADO].keys())
    
    for i, param_name in enumerate(param_names):
        if i < len(vector_codificado):
            opciones = HIPERPARAMETROS[ALGORITMO_SELECCIONADO][param_name]
            idx = int(vector_codificado[i]) % len(opciones)  # Asegurar √≠ndice v√°lido
            hiperparametros[param_name] = opciones[idx]
        else:
            # Si no hay valor codificado, usar el primero de las opciones
            opciones = HIPERPARAMETROS[ALGORITMO_SELECCIONADO][param_name]
            hiperparametros[param_name] = opciones[0]
    
    return hiperparametros

def crear_modelo_con_hiperparametros(hiperparametros_dict):
    """
    Crea un modelo con los hiperpar√°metros especificados.
    
    Args:
        hiperparametros_dict: Diccionario con hiperpar√°metros
    
    Returns:
        Modelo de sklearn configurado
    """
    if ALGORITMO_SELECCIONADO == 'LogisticRegression':
        return LogisticRegression(random_state=RANDOM_STATE, **hiperparametros_dict)
    elif ALGORITMO_SELECCIONADO == 'RandomForestClassifier':
        return RandomForestClassifier(random_state=RANDOM_STATE, **hiperparametros_dict)
    elif ALGORITMO_SELECCIONADO == 'SVC':
        return SVC(random_state=RANDOM_STATE, **hiperparametros_dict)
    else:
        raise ValueError(f"Algoritmo no soportado: {ALGORITMO_SELECCIONADO}")

# ===================================================================
# 5. FUNCI√ìN DE EVALUACI√ìN (FITNESS)
# ===================================================================
# Eval√∫a qu√© tan bueno es un conjunto de hiperpar√°metros

def evaluar_hiperparametros(vector_codificado):
    """
    Eval√∫a la calidad de un conjunto de hiperpar√°metros usando F1-Score con CV.
    
    Args:
        vector_codificado: Vector codificado de hiperpar√°metros
    
    Returns:
        float: F1-Score promedio obtenido con validaci√≥n cruzada
    """
    try:
        # Decodificar hiperpar√°metros
        hiperparametros = decodificar_hiperparametros(vector_codificado)
        
        # Crear modelo con los hiperpar√°metros
        modelo = crear_modelo_con_hiperparametros(hiperparametros)
        
        # Configurar validaci√≥n cruzada estratificada
        skf = StratifiedKFold(n_splits=KFOLD, shuffle=True, random_state=RANDOM_STATE)
        
        f1_scores = []
        
        # Evaluar en cada fold
        for train_idx, val_idx in skf.split(X, y):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            # Entrenar modelo
            modelo.fit(X_train, y_train)
            
            # Hacer predicciones y calcular F1-Score
            predictions = modelo.predict(X_val)
            f1 = f1_score(y_val, predictions, average="macro")
            f1_scores.append(f1)
        
        return float(np.mean(f1_scores))
    
    except Exception as e:
        # Si hay error en el entrenamiento, retornar fitness muy bajo
        print(f"‚ö†Ô∏è Error en evaluaci√≥n: {e}")
        return 0.0

# ===================================================================
# 6. OPERADORES GEN√âTICOS
# ===================================================================
# Implementaci√≥n de los operadores b√°sicos del algoritmo gen√©tico

def crear_individuo_aleatorio():
    """
    Crea un individuo aleatorio (conjunto de hiperpar√°metros).
    
    Returns:
        numpy.array: Vector codificado de hiperpar√°metros aleatorios
    """
    vector = []
    
    for param_name in HIPERPARAMETROS[ALGORITMO_SELECCIONADO].keys():
        opciones = HIPERPARAMETROS[ALGORITMO_SELECCIONADO][param_name]
        idx_aleatorio = np.random.randint(0, len(opciones))
        vector.append(idx_aleatorio)
    
    return np.array(vector)

def seleccion_por_torneo(poblacion, fitnesses, k=TORNEO_K):
    """
    Selecciona un individuo usando torneo de tama√±o k.
    
    Args:
        poblacion: Lista de individuos (vectores de hiperpar√°metros)
        fitnesses: Lista de valores de fitness correspondientes
        k: Tama√±o del torneo
    
    Returns:
        numpy.array: Individuo seleccionado (copia)
    """
    # Seleccionar k individuos aleatorios
    indices_competidores = np.random.choice(len(poblacion), size=k, replace=False)
    
    # Encontrar el competidor con mejor fitness
    fitnesses_competidores = [fitnesses[i] for i in indices_competidores]
    ganador_idx = indices_competidores[np.argmax(fitnesses_competidores)]
    
    return poblacion[ganador_idx].copy()

def cruce_aritmetico(padre, madre):
    """
    Realiza cruce aritm√©tico entre dos padres.
    
    Args:
        padre: Cromosoma del primer padre
        madre: Cromosoma del segundo padre
    
    Returns:
        tuple: (hijo1, hijo2) - Los dos descendientes
    """
    if np.random.rand() > PROB_CRUCE or len(padre) < 2:
        return padre.copy(), madre.copy()
    
    # Cruce aritm√©tico: promedio ponderado
    alpha = np.random.rand()  # Peso aleatorio
    
    hijo1 = alpha * padre + (1 - alpha) * madre
    hijo2 = (1 - alpha) * padre + alpha * madre
    
    # Redondear a enteros (ya que son √≠ndices)
    hijo1 = np.round(hijo1).astype(int)
    hijo2 = np.round(hijo2).astype(int)
    
    # Asegurar que los valores est√©n en rangos v√°lidos
    for i, param_name in enumerate(HIPERPARAMETROS[ALGORITMO_SELECCIONADO].keys()):
        max_val = len(HIPERPARAMETROS[ALGORITMO_SELECCIONADO][param_name]) - 1
        hijo1[i] = max(0, min(hijo1[i], max_val))
        hijo2[i] = max(0, min(hijo2[i], max_val))
    
    return hijo1, hijo2

def mutacion_gaussiana(individuo):
    """
    Aplica mutaci√≥n gaussiana a un individuo.
    
    Args:
        individuo: Cromosoma a mutar
    
    Returns:
        numpy.array: Cromosoma mutado
    """
    individuo_mutado = individuo.copy()
    
    for i, param_name in enumerate(HIPERPARAMETROS[ALGORITMO_SELECCIONADO].keys()):
        if np.random.rand() < PROB_MUTACION:
            # Mutaci√≥n gaussiana
            max_val = len(HIPERPARAMETROS[ALGORITMO_SELECCIONADO][param_name]) - 1
            mutacion = np.random.normal(0, 0.5)  # Desviaci√≥n est√°ndar peque√±a
            
            nuevo_valor = individuo_mutado[i] + mutacion
            nuevo_valor = int(np.round(nuevo_valor))
            
            # Asegurar que est√© en rango v√°lido
            individuo_mutado[i] = max(0, min(nuevo_valor, max_val))
    
    return individuo_mutado

# ===================================================================
# 7. ALGORITMO GEN√âTICO PRINCIPAL
# ===================================================================
# Ejecuta el algoritmo gen√©tico para optimizaci√≥n de hiperpar√°metros

def ejecutar_algoritmo_genetico():
    """
    Ejecuta el algoritmo gen√©tico para optimizaci√≥n de hiperpar√°metros.
    
    Returns:
        tuple: (mejor_hiperparametros, mejor_fitness)
    """
    
    print("\nüß¨ INICIANDO ALGORITMO GEN√âTICO...")
    
    # ===================================================================
    # 7.1 INICIALIZACI√ìN DE LA POBLACI√ìN
    # ===================================================================
    print("üìã Generando poblaci√≥n inicial...")
    
    # Crear poblaci√≥n inicial aleatoria
    poblacion = [crear_individuo_aleatorio() for _ in range(POBLACION_INICIAL)]
    
    # Evaluar fitness de cada individuo
    print("‚ö° Evaluando fitness de poblaci√≥n inicial...")
    fitnesses = [evaluar_hiperparametros(ind) for ind in poblacion]
    
    # Encontrar el mejor individuo inicial
    mejor_individuo = poblacion[int(np.argmax(fitnesses))].copy()
    mejor_fitness = float(np.max(fitnesses))
    mejor_hiperparametros = decodificar_hiperparametros(mejor_individuo)
    
    print(f"üéØ Generaci√≥n 0 | Mejor F1-Score: {mejor_fitness:.4f}")
    print(f"   Hiperpar√°metros: {mejor_hiperparametros}")
    
    # ===================================================================
    # 7.2 EVOLUCI√ìN POR GENERACIONES
    # ===================================================================
    for generacion in range(1, GENERACIONES + 1):
        print(f"üîÑ Procesando generaci√≥n {generacion}...")
        
        nueva_poblacion = []
        
        # Crear nueva poblaci√≥n
        while len(nueva_poblacion) < POBLACION_INICIAL:
            # Selecci√≥n de padres
            padre1 = seleccion_por_torneo(poblacion, fitnesses)
            padre2 = seleccion_por_torneo(poblacion, fitnesses)
            
            # Cruce y mutaci√≥n
            hijo1, hijo2 = cruce_aritmetico(padre1, padre2)
            hijo1 = mutacion_gaussiana(hijo1)
            hijo2 = mutacion_gaussiana(hijo2)
            
            nueva_poblacion.extend([hijo1, hijo2])
        
        # Asegurar tama√±o correcto de poblaci√≥n
        poblacion = nueva_poblacion[:POBLACION_INICIAL]
        
        # Evaluar nueva poblaci√≥n
        fitnesses = [evaluar_hiperparametros(ind) for ind in poblacion]
        
        # Actualizar mejor individuo
        idx_mejor_actual = int(np.argmax(fitnesses))
        if fitnesses[idx_mejor_actual] > mejor_fitness:
            mejor_fitness = float(fitnesses[idx_mejor_actual])
            mejor_individuo = poblacion[idx_mejor_actual].copy()
            mejor_hiperparametros = decodificar_hiperparametros(mejor_individuo)
        
        print(f"üéØ Generaci√≥n {generacion} | Mejor F1-Score: {mejor_fitness:.4f}")
        print(f"   Hiperpar√°metros: {mejor_hiperparametros}")
    
    print("‚úÖ Algoritmo gen√©tico completado!")
    return mejor_hiperparametros, mejor_fitness

# ===================================================================
# 8. EJECUCI√ìN DEL ALGORITMO
# ===================================================================
# Ejecutar el algoritmo gen√©tico y obtener resultados

mejor_hiperparametros, mejor_score = ejecutar_algoritmo_genetico()

# ===================================================================
# 9. AN√ÅLISIS DE RESULTADOS
# ===================================================================
# Analizar y mostrar los resultados obtenidos

print("\n" + "=" * 60)
print("üìä RESULTADOS DE LA OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS")
print("=" * 60)

print(f"üéØ Mejor F1-Score obtenido: {mejor_score:.4f}")
print(f"ü§ñ Algoritmo optimizado: {ALGORITMO_SELECCIONADO}")

print(f"\n‚úÖ HIPERPAR√ÅMETROS √ìPTIMOS:")
for param_name, param_value in mejor_hiperparametros.items():
    print(f"   {param_name}: {param_value}")

# ===================================================================
# 10. ENTRENAMIENTO DEL MODELO FINAL
# ===================================================================
# Entrenar el modelo final con los hiperpar√°metros √≥ptimos

print(f"\nü§ñ ENTRENANDO MODELO FINAL CON HIPERPAR√ÅMETROS √ìPTIMOS...")

# Crear modelo con hiperpar√°metros √≥ptimos
modelo_final = crear_modelo_con_hiperparametros(mejor_hiperparametros)

# Entrenar con todos los datos
modelo_final.fit(X, y)

# Hacer predicciones y generar reporte
predicciones_finales = modelo_final.predict(X)

print("\n" + "=" * 60)
print("üìã REPORTE DE CLASIFICACI√ìN FINAL")
print("=" * 60)
print(classification_report(y, predicciones_finales, digits=4))

# ===================================================================
# 11. COMPARACI√ìN CON CONFIGURACI√ìN POR DEFECTO
# ===================================================================
# Comparar con el modelo usando hiperpar√°metros por defecto

print(f"\nüîç COMPARACI√ìN CON CONFIGURACI√ìN POR DEFECTO...")

# Crear modelo con configuraci√≥n por defecto
if ALGORITMO_SELECCIONADO == 'LogisticRegression':
    modelo_default = LogisticRegression(random_state=RANDOM_STATE)
elif ALGORITMO_SELECCIONADO == 'RandomForestClassifier':
    modelo_default = RandomForestClassifier(random_state=RANDOM_STATE)
elif ALGORITMO_SELECCIONADO == 'SVC':
    modelo_default = SVC(random_state=RANDOM_STATE)

# Entrenar y evaluar modelo por defecto
modelo_default.fit(X, y)
predicciones_default = modelo_default.predict(X)
f1_default = f1_score(y, predicciones_default, average="macro")

print(f"üìä F1-Score con hiperpar√°metros por defecto: {f1_default:.4f}")
print(f"üìä F1-Score con hiperpar√°metros optimizados: {mejor_score:.4f}")
print(f"üìà Mejora obtenida: {((mejor_score - f1_default) / f1_default * 100):.2f}%")

# ===================================================================
# 12. GUARDAR RESULTADOS
# ===================================================================
# Guardar los resultados en archivos

print(f"\nüíæ GUARDANDO RESULTADOS...")

# Crear DataFrame con los resultados
resultados_df = pd.DataFrame([mejor_hiperparametros])
resultados_df['f1_score'] = mejor_score
resultados_df['algoritmo'] = ALGORITMO_SELECCIONADO

# Guardar resultados
resultados_df.to_csv("hiperparametros_optimizados_ag.csv", index=False)

print("‚úÖ Archivo generado:")
print("   üìÑ hiperparametros_optimizados_ag.csv")

# ===================================================================
# 13. RESUMEN FINAL
# ===================================================================
print("\n" + "=" * 60)
print("üéâ RESUMEN FINAL")
print("=" * 60)
print(f"üéØ Algoritmo optimizado: {ALGORITMO_SELECCIONADO}")
print(f"üìä F1-Score obtenido: {mejor_score:.4f}")
print(f"üìà Mejora vs configuraci√≥n por defecto: {((mejor_score - f1_default) / f1_default * 100):.2f}%")
print(f"üîß Hiperpar√°metros optimizados: {len(mejor_hiperparametros)} par√°metros")
print("=" * 60)

# ===================================================================
# EXPLICACI√ìN DE LA METODOLOG√çA
# ===================================================================
"""
EXPLICACI√ìN DETALLADA DE LA METODOLOG√çA:

1. REPRESENTACI√ìN DEL PROBLEMA:
   - Cada cromosoma es un vector num√©rico que representa √≠ndices de hiperpar√°metros
   - Cada posici√≥n del vector corresponde a un hiperpar√°metro espec√≠fico
   - El valor en cada posici√≥n es el √≠ndice de la opci√≥n seleccionada

2. ESPACIO DE B√öSQUEDA:
   - LogisticRegression: C, penalty, solver, max_iter
   - RandomForestClassifier: n_estimators, max_depth, min_samples_split, etc.
   - SVC: C, kernel, gamma, degree
   - Cada hiperpar√°metro tiene un conjunto discreto de valores posibles

3. FUNCI√ìN DE EVALUACI√ìN:
   - Usa validaci√≥n cruzada estratificada para evaluar cada configuraci√≥n
   - Entrena el modelo con los hiperpar√°metros especificados
   - Calcula F1-Score macro promedio de todos los folds
   - Maneja errores de entrenamiento retornando fitness bajo

4. OPERADORES GEN√âTICOS:
   - SELECCI√ìN: Torneo de tama√±o k
   - CRUCE: Cruce aritm√©tico (promedio ponderado de padres)
   - MUTACI√ìN: Mutaci√≥n gaussiana (peque√±os cambios aleatorios)

5. VENTAJAS DE ESTE ENFOQUE:
   - Explora m√∫ltiples configuraciones simult√°neamente
   - No requiere gradientes (√∫til para algoritmos no diferenciables)
   - Puede manejar espacios de b√∫squeda discretos y continuos
   - Encuentra configuraciones que funcionan bien en conjunto

6. DIFERENCIAS CON FEATURE SELECTION:
   - Representaci√≥n: √çndices en lugar de binarios
   - Operadores: Cruce aritm√©tico y mutaci√≥n gaussiana
   - Espacio de b√∫squeda: Hiperpar√°metros vs caracter√≠sticas
   - Objetivo: Optimizar configuraci√≥n del modelo vs seleccionar variables

7. APLICACIONES:
   - Optimizaci√≥n de cualquier algoritmo de ML
   - Tuning autom√°tico de hiperpar√°metros
   - Mejora del rendimiento de modelos
   - Reducci√≥n del tiempo de experimentaci√≥n manual
"""
