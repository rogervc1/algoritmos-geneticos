# algoritmos-geneticos
# ğŸ§¬ Proyecto de Algoritmos GenÃ©ticos para Machine Learning

## ğŸ“‹ DescripciÃ³n General

Este proyecto implementa **Algoritmos GenÃ©ticos** para resolver dos problemas fundamentales en Machine Learning:

1. **SelecciÃ³n de CaracterÃ­sticas (Feature Selection)**: Encontrar el subconjunto Ã³ptimo de variables predictoras
2. **OptimizaciÃ³n de HiperparÃ¡metros**: Encontrar la mejor configuraciÃ³n de parÃ¡metros para modelos de ML

El proyecto utiliza un dataset mÃ©dico sobre **predicciÃ³n de accidentes cerebrovasculares (stroke)** para demostrar la efectividad de estos algoritmos evolutivos.

---

## ğŸ¯ Objetivos del Proyecto

### Problema Principal
- **Dataset**: PredicciÃ³n de accidentes cerebrovasculares
- **Variable Objetivo**: `stroke` (0 = no tuvo ACV, 1 = sÃ­ tuvo ACV)
- **DesafÃ­o**: Optimizar el rendimiento del modelo de clasificaciÃ³n

### Objetivos EspecÃ­ficos
1. **Reducir dimensionalidad** eliminando caracterÃ­sticas irrelevantes
2. **Mejorar precisiÃ³n** optimizando hiperparÃ¡metros del modelo
3. **Automatizar** el proceso de optimizaciÃ³n usando algoritmos evolutivos
4. **Comparar** rendimiento vs configuraciÃ³n por defecto

---

## ğŸ“ Estructura del Proyecto

```
completo_final/
â”œâ”€â”€ ğŸ“„ ag_feature_selection.py          # Algoritmo genÃ©tico para selecciÃ³n de caracterÃ­sticas
â”œâ”€â”€ ğŸ“„ ag_hyperparameter_optimization.py # Algoritmo genÃ©tico para optimizaciÃ³n de hiperparÃ¡metros
â”œâ”€â”€ ğŸ“„ ag_neuroevolution.py             # Algoritmo genÃ©tico para hallar la mejor arquitectura de una RN
â”œâ”€â”€ ğŸ“„ convertir.py                     # Script de conversiÃ³n de datos
â”œâ”€â”€ ğŸ“„ dataset_1_convertido.csv         # Dataset procesado y binarizado
â”œâ”€â”€ ğŸ“„ dataset_reducido_ag.csv          # Dataset con caracterÃ­sticas seleccionadas
â”œâ”€â”€ ğŸ“„ caracteristicas_seleccionadas_ag.csv # Lista de caracterÃ­sticas Ã³ptimas
â”œâ”€â”€ ğŸ“„ hiperparametros_optimizados_ag.csv   # HiperparÃ¡metros Ã³ptimos encontrados
â””â”€â”€ ğŸ“ conversion/
    â”œâ”€â”€ ğŸ“„ dataset_1.csv                # Dataset original
    â”œâ”€â”€ ğŸ“„ dataset_1_convertido.csv     # Dataset convertido
    â””â”€â”€ ğŸ“„ convertir.py                 # Script de conversiÃ³n
```

---

## ğŸ”¬ MetodologÃ­a CientÃ­fica

### 1. PreparaciÃ³n de Datos (`convertir.py`)

**Objetivo**: Convertir variables categÃ³ricas a formato binario para algoritmos genÃ©ticos.

**Transformaciones aplicadas**:
- **GÃ©nero**: `Male` â†’ 1, `Female` â†’ 0
- **Edad**: â‰¥30 aÃ±os â†’ 1, <30 aÃ±os â†’ 0
- **Estado civil**: `Yes` (casado) â†’ 1, `No` â†’ 0
- **Tipo de trabajo**: `Private` â†’ 1, otros â†’ 0
- **Residencia**: `Urban` â†’ 1, `Rural` â†’ 0
- **Glucosa**: >100 â†’ 1, â‰¤100 â†’ 0
- **BMI**: 18.5-24.9 (saludable) â†’ 1, otros â†’ 0
- **EliminaciÃ³n**: Columna `smoking_status` (no relevante)

**Resultado**: Dataset binario con 10 caracterÃ­sticas + variable objetivo.

### 2. SelecciÃ³n de CaracterÃ­sticas (`ag_feature_selection.py`)

**Problema**: Â¿CuÃ¡les de las 10 caracterÃ­sticas son realmente importantes para predecir stroke?

**SoluciÃ³n**: Algoritmo GenÃ©tico que explora combinaciones de caracterÃ­sticas.

#### ğŸ”§ ConfiguraciÃ³n del Algoritmo GenÃ©tico
```python
POBLACION_INICIAL = 30      # 30 individuos por generaciÃ³n
GENERACIONES = 25           # 25 iteraciones evolutivas
PROB_CRUCE = 0.8           # 80% probabilidad de cruce
PROB_MUTACION = 0.05       # 5% probabilidad de mutaciÃ³n
TORNEO_K = 3               # Torneo de 3 individuos
KFOLD = 5                  # ValidaciÃ³n cruzada 5-fold
```

#### ğŸ§¬ RepresentaciÃ³n GenÃ©tica
- **Cromosoma**: Vector binario de longitud 10
- **Gen**: 1 = caracterÃ­stica seleccionada, 0 = descartada
- **Ejemplo**: `[1,0,1,0,1,0,0,1,0,1]` â†’ selecciona caracterÃ­sticas 1,3,5,8,10

#### âš¡ FunciÃ³n de EvaluaciÃ³n (Fitness)
```python
def evaluar_cromosoma(mask):
    # 1. Seleccionar caracterÃ­sticas marcadas con 1
    X_seleccionado = X.loc[:, mask.astype(bool)]
    
    # 2. ValidaciÃ³n cruzada estratificada (5-fold)
    skf = StratifiedKFold(n_splits=5, shuffle=True)
    
    # 3. Entrenar modelo en cada fold
    for train_idx, val_idx in skf.split(X_seleccionado, y):
        modelo = LogisticRegression(max_iter=200)
        modelo.fit(X_train, y_train)
        predicciones = modelo.predict(X_val)
        f1 = f1_score(y_val, predicciones, average="macro")
    
    # 4. Retornar F1-Score promedio
    return np.mean(f1_scores)
```

#### ğŸ”„ Operadores GenÃ©ticos

**1. SelecciÃ³n por Torneo**:
```python
def seleccion_por_torneo(poblacion, fitnesses, k=3):
    # Seleccionar 3 individuos aleatorios
    competidores = np.random.choice(len(poblacion), size=3)
    # Retornar el de mejor fitness
    return poblacion[mejor_competidor]
```

**2. Cruce de Un Punto**:
```python
def cruce_de_un_punto(padre, madre):
    punto_cruce = np.random.randint(1, len(padre))
    hijo1 = np.concatenate([padre[:punto_cruce], madre[punto_cruce:]])
    hijo2 = np.concatenate([madre[:punto_cruce], padre[punto_cruce:]])
    return hijo1, hijo2
```

**3. MutaciÃ³n Bit-Flip**:
```python
def mutacion_bit_flip(individuo):
    mascara = np.random.rand(len(individuo)) < PROB_MUTACION
    individuo_mutado[mascara] = 1 - individuo_mutado[mascara]
    return individuo_mutado
```

#### ğŸ“Š Resultados Obtenidos
- **CaracterÃ­sticas seleccionadas**: 5 de 10 (50% reducciÃ³n)
- **F1-Score**: 0.7455
- **CaracterÃ­sticas Ã³ptimas**: `age`, `ever_married`, `work_type`, `Residence_type`, `avg_glucose_level`

### 3. OptimizaciÃ³n de HiperparÃ¡metros (`ag_hyperparameter_optimization.py`)

**Problema**: Â¿CuÃ¡les son los mejores valores para los hiperparÃ¡metros del modelo?

**SoluciÃ³n**: Algoritmo GenÃ©tico que explora el espacio de hiperparÃ¡metros.

#### ğŸ”§ ConfiguraciÃ³n del Algoritmo GenÃ©tico
```python
POBLACION_INICIAL = 20      # Menor poblaciÃ³n (mÃ¡s costoso evaluar)
GENERACIONES = 15           # Menos generaciones
PROB_CRUCE = 0.8           # 80% probabilidad de cruce
PROB_MUTACION = 0.1        # 10% probabilidad de mutaciÃ³n (mayor)
```

#### ğŸ§¬ Espacio de HiperparÃ¡metros
**LogisticRegression**:
- `C`: [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] (regularizaciÃ³n)
- `penalty`: ['l1', 'l2'] (tipo de regularizaciÃ³n)
- `solver`: ['liblinear', 'saga'] (algoritmo de optimizaciÃ³n)
- `max_iter`: [100, 200, 500, 1000] (mÃ¡ximo iteraciones)

#### ğŸ”„ Operadores GenÃ©ticos Especializados

**1. Cruce AritmÃ©tico**:
```python
def cruce_aritmetico(padre, madre):
    alpha = np.random.rand()  # Peso aleatorio
    hijo1 = alpha * padre + (1 - alpha) * madre
    hijo2 = (1 - alpha) * padre + alpha * madre
    return np.round(hijo1).astype(int), np.round(hijo2).astype(int)
```

**2. MutaciÃ³n Gaussiana**:
```python
def mutacion_gaussiana(individuo):
    for i in range(len(individuo)):
        if np.random.rand() < PROB_MUTACION:
            mutacion = np.random.normal(0, 0.5)
            individuo[i] = max(0, min(individuo[i] + mutacion, max_val))
    return individuo
```

#### ğŸ“Š Resultados Obtenidos
- **HiperparÃ¡metros Ã³ptimos**: `C=1.0`, `penalty='l2'`, `solver='saga'`, `max_iter=500`
- **F1-Score**: 0.7455
- **Mejora vs configuraciÃ³n por defecto**: Variable segÃºn el dataset

---

## ğŸ§ª Bases TeÃ³ricas y Fundamentos

### Algoritmos GenÃ©ticos
Los algoritmos genÃ©ticos son **algoritmos de optimizaciÃ³n** inspirados en la evoluciÃ³n biolÃ³gica:

1. **PoblaciÃ³n**: Conjunto de soluciones candidatas
2. **SelecciÃ³n**: Los mejores individuos tienen mÃ¡s probabilidad de reproducirse
3. **Cruce**: CombinaciÃ³n de caracterÃ­sticas de padres para crear hijos
4. **MutaciÃ³n**: IntroducciÃ³n de variaciÃ³n aleatoria
5. **EvaluaciÃ³n**: FunciÃ³n objetivo que mide la calidad de cada soluciÃ³n

### Ventajas de los Algoritmos GenÃ©ticos
- âœ… **ExploraciÃ³n global**: No se quedan atrapados en Ã³ptimos locales
- âœ… **Paralelizable**: Pueden evaluar mÃºltiples soluciones simultÃ¡neamente
- âœ… **Robustos**: Funcionan bien con funciones no diferenciables
- âœ… **Flexibles**: Se adaptan a diferentes tipos de problemas

### MÃ©tricas de EvaluaciÃ³n
- **F1-Score Macro**: Promedio no ponderado de precisiÃ³n y recall
- **ValidaciÃ³n Cruzada**: EvalÃºa robustez del modelo
- **EstratificaciÃ³n**: Mantiene proporciÃ³n de clases en cada fold

---

## ğŸ“ˆ Resultados y AnÃ¡lisis

### 1. SelecciÃ³n de CaracterÃ­sticas

**CaracterÃ­sticas seleccionadas** (5 de 10):
1. `age` - Edad (â‰¥30 aÃ±os)
2. `ever_married` - Estado civil (casado)
3. `work_type` - Tipo de trabajo (privado)
4. `Residence_type` - Tipo de residencia (urbana)
5. `avg_glucose_level` - Nivel de glucosa (>100)

**CaracterÃ­sticas descartadas** (5 de 10):
1. `gender` - GÃ©nero
2. `hypertension` - HipertensiÃ³n
3. `heart_disease` - Enfermedad cardÃ­aca
4. `bmi` - Ãndice de masa corporal

**InterpretaciÃ³n mÃ©dica**:
- âœ… **Edad**: Factor de riesgo principal para ACV
- âœ… **Estado civil**: Posible indicador de estrÃ©s/estilo de vida
- âœ… **Trabajo privado**: Posible indicador de estrÃ©s laboral
- âœ… **Residencia urbana**: Mayor exposiciÃ³n a factores de riesgo
- âœ… **Glucosa alta**: Factor de riesgo cardiovascular

### 2. OptimizaciÃ³n de HiperparÃ¡metros

**ConfiguraciÃ³n Ã³ptima**:
- `C=1.0`: RegularizaciÃ³n moderada
- `penalty='l2'`: RegularizaciÃ³n L2 (Ridge)
- `solver='saga'`: Algoritmo eficiente para datasets pequeÃ±os
- `max_iter=500`: Suficientes iteraciones para convergencia

**InterpretaciÃ³n tÃ©cnica**:
- âœ… **C=1.0**: Balance entre sesgo y varianza
- âœ… **L2 penalty**: Penaliza coeficientes grandes, previene overfitting
- âœ… **SAGA solver**: Eficiente para regularizaciÃ³n L1 y L2
- âœ… **500 iteraciones**: Garantiza convergencia sin sobre-entrenamiento

---

## ğŸš€ CÃ³mo Ejecutar el Proyecto

### Prerrequisitos
```bash
pip install numpy pandas scikit-learn
```

### 1. Preparar Datos
```bash
cd conversion/
python convertir.py
```

### 2. SelecciÃ³n de CaracterÃ­sticas
```bash
python ag_feature_selection.py
```

### 3. OptimizaciÃ³n de HiperparÃ¡metros
```bash
python ag_hyperparameter_optimization.py
```

### Archivos Generados
- `caracteristicas_seleccionadas_ag.csv`: Lista de caracterÃ­sticas Ã³ptimas
- `dataset_reducido_ag.csv`: Dataset con caracterÃ­sticas seleccionadas
- `hiperparametros_optimizados_ag.csv`: HiperparÃ¡metros Ã³ptimos

---

## ğŸ” InterpretaciÃ³n de Resultados

### Archivos de Salida

#### `caracteristicas_seleccionadas_ag.csv`
```csv
caracteristica
age
ever_married
work_type
Residence_type
avg_glucose_level
```
**Significado**: Lista de las 5 caracterÃ­sticas mÃ¡s importantes para predecir stroke.

#### `dataset_reducido_ag.csv`
```csv
age,ever_married,work_type,Residence_type,avg_glucose_level,stroke
1,0,0,1,1,1
1,0,0,0,1,1
...
```
**Significado**: Dataset optimizado con solo las caracterÃ­sticas relevantes.

#### `hiperparametros_optimizados_ag.csv`
```csv
C,penalty,solver,max_iter,f1_score,algoritmo
1.0,l2,saga,500,0.7455122655122655,LogisticRegression
```
**Significado**: ConfiguraciÃ³n Ã³ptima del modelo con su rendimiento.

---

## ğŸ“ Aplicaciones y Casos de Uso

### SelecciÃ³n de CaracterÃ­sticas
- **Medicina**: Identificar biomarcadores mÃ¡s relevantes
- **Finanzas**: Seleccionar indicadores econÃ³micos predictivos
- **Marketing**: Encontrar variables demogrÃ¡ficas clave
- **IoT**: Reducir sensores necesarios para predicciÃ³n

### OptimizaciÃ³n de HiperparÃ¡metros
- **Deep Learning**: Optimizar arquitecturas de redes neuronales
- **Sistemas de RecomendaciÃ³n**: Ajustar parÃ¡metros de algoritmos
- **Procesamiento de ImÃ¡genes**: Optimizar filtros y transformaciones
- **NLP**: Ajustar parÃ¡metros de modelos de lenguaje

---

## ğŸ”¬ MetodologÃ­a CientÃ­fica Detallada

### DiseÃ±o Experimental
1. **HipÃ³tesis**: Los algoritmos genÃ©ticos pueden encontrar configuraciones Ã³ptimas
2. **Variables independientes**: CaracterÃ­sticas e hiperparÃ¡metros
3. **Variable dependiente**: F1-Score del modelo
4. **Control**: ComparaciÃ³n con configuraciÃ³n por defecto
5. **Reproducibilidad**: Semilla aleatoria fija (RANDOM_STATE=42)

### ValidaciÃ³n
- **ValidaciÃ³n cruzada**: 5-fold estratificado
- **MÃ©tricas robustas**: F1-Score macro
- **MÃºltiples evaluaciones**: Cada cromosoma evaluado 5 veces
- **Manejo de errores**: Fitness bajo para configuraciones invÃ¡lidas

### Limitaciones
- **TamaÃ±o de poblaciÃ³n**: Limitado por recursos computacionales
- **Generaciones**: NÃºmero fijo, no adaptativo
- **Espacio de bÃºsqueda**: Discreto para hiperparÃ¡metros
- **Dataset pequeÃ±o**: 42 muestras, puede limitar generalizaciÃ³n

---

## ğŸ† Conclusiones

### Logros del Proyecto
1. âœ… **ReducciÃ³n efectiva**: 50% menos caracterÃ­sticas manteniendo rendimiento
2. âœ… **OptimizaciÃ³n automÃ¡tica**: EncontrÃ³ hiperparÃ¡metros Ã³ptimos sin intervenciÃ³n manual
3. âœ… **MetodologÃ­a robusta**: ValidaciÃ³n cruzada y mÃ©tricas apropiadas
4. âœ… **Interpretabilidad**: Resultados mÃ©dicamente coherentes

### Impacto CientÃ­fico
- **Eficiencia computacional**: Menos caracterÃ­sticas = modelos mÃ¡s rÃ¡pidos
- **Interpretabilidad**: CaracterÃ­sticas seleccionadas tienen sentido mÃ©dico
- **AutomatizaciÃ³n**: Reduce tiempo de experimentaciÃ³n manual
- **Escalabilidad**: MetodologÃ­a aplicable a otros problemas

### Futuras Mejoras
- **PoblaciÃ³n adaptativa**: Ajustar tamaÃ±o segÃºn convergencia
- **Operadores avanzados**: Cruce uniforme, mutaciÃ³n adaptativa
- **MÃºltiples objetivos**: Optimizar precisiÃ³n y velocidad simultÃ¡neamente
- **Ensemble methods**: Combinar mÃºltiples algoritmos genÃ©ticos

---

## ğŸ“š Referencias y Fundamentos

### Algoritmos GenÃ©ticos
- Holland, J.H. (1975). "Adaptation in Natural and Artificial Systems"
- Goldberg, D.E. (1989). "Genetic Algorithms in Search, Optimization, and Machine Learning"

### Machine Learning
- Hastie, T., Tibshirani, R., & Friedman, J. (2009). "The Elements of Statistical Learning"
- Bishop, C.M. (2006). "Pattern Recognition and Machine Learning"

### Feature Selection
- Guyon, I., & Elisseeff, A. (2003). "An introduction to variable and feature selection"
- Saeys, Y., et al. (2007). "A review of feature selection techniques in bioinformatics"

---

## ğŸ‘¨â€ğŸ’» Autor

**Proyecto desarrollado para demostrar la aplicaciÃ³n de Algoritmos GenÃ©ticos en problemas de Machine Learning**

*MetodologÃ­a implementada siguiendo las mejores prÃ¡cticas de la literatura cientÃ­fica en algoritmos evolutivos y machine learning.*
