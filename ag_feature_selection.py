# ===================================================================
# ALGORITMO GEN√âTICO PARA SELECCI√ìN DE CARACTER√çSTICAS (FEATURE SELECTION)
# ===================================================================
# 
# OBJETIVO: Encontrar el subconjunto √≥ptimo de caracter√≠sticas que
# maximiza el rendimiento de un modelo de machine learning.
#
# PROBLEMA: De todas las caracter√≠sticas disponibles en el dataset,
# ¬øcu√°les son las m√°s importantes para predecir la variable objetivo?
#
# SOLUCI√ìN: Usar un Algoritmo Gen√©tico para explorar diferentes
# combinaciones de caracter√≠sticas y encontrar la mejor.
# ===================================================================

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import f1_score, classification_report
from sklearn.exceptions import ConvergenceWarning
import warnings
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# ===================================================================
# 1. CONFIGURACI√ìN DEL ALGORITMO GEN√âTICO
# ===================================================================
# Estos par√°metros controlan el comportamiento del algoritmo gen√©tico

RANDOM_STATE = 42           # Semilla para reproducibilidad
POBLACION_INICIAL = 30      # Tama√±o de la poblaci√≥n (n√∫mero de individuos)
GENERACIONES = 25           # N√∫mero de generaciones (iteraciones)
PROB_CRUCE = 0.8           # Probabilidad de cruce entre padres (80%)
PROB_MUTACION = 0.05       # Probabilidad de mutaci√≥n por gen (5%)
TORNEO_K = 3               # Tama√±o del torneo para selecci√≥n
KFOLD = 5                  # N√∫mero de folds para validaci√≥n cruzada

# Establecer semilla aleatoria para resultados reproducibles
np.random.seed(RANDOM_STATE)

print("=" * 60)
print("ALGORITMO GEN√âTICO PARA SELECCI√ìN DE CARACTER√çSTICAS")
print("=" * 60)
print(f"Poblaci√≥n: {POBLACION_INICIAL} individuos")
print(f"Generaciones: {GENERACIONES}")
print(f"Probabilidad de cruce: {PROB_CRUCE}")
print(f"Probabilidad de mutaci√≥n: {PROB_MUTACION}")
print(f"Validaci√≥n cruzada: {KFOLD}-fold")
print("=" * 60)

# ===================================================================
# 2. CARGA Y PREPARACI√ìN DE DATOS
# ===================================================================
# Cargamos el dataset y lo preparamos para el algoritmo

print("\nüìä CARGANDO DATOS...")
df = pd.read_csv("dataset_1_convertido.csv")
df.columns = df.columns.str.strip()  # Limpiar espacios en nombres de columnas

# Separar caracter√≠sticas (X) y variable objetivo (y)
if "stroke" not in df.columns:
    raise ValueError("No se encontr√≥ la columna objetivo 'stroke' en el dataset.")

y = df["stroke"].astype(int)  # Variable objetivo: 0 = no stroke, 1 = stroke

# Excluir columnas que no son caracter√≠sticas predictoras
cols_excluir = [c for c in ["stroke", "id"] if c in df.columns]
X = df.drop(columns=cols_excluir)  # Caracter√≠sticas predictoras

feature_names = X.columns.tolist()  # Nombres de las caracter√≠sticas
n_features = X.shape[1]            # N√∫mero total de caracter√≠sticas

print(f"‚úÖ Dataset cargado: {X.shape[0]} muestras, {n_features} caracter√≠sticas")
print(f"‚úÖ Variable objetivo: {y.value_counts().to_dict()}")
print(f"‚úÖ Caracter√≠sticas disponibles: {feature_names}")

# ===================================================================
# 3. FUNCI√ìN DE EVALUACI√ìN (FITNESS)
# ===================================================================
# Esta funci√≥n eval√∫a qu√© tan bueno es un cromosoma (combinaci√≥n de caracter√≠sticas)

def evaluar_cromosoma(mask):
    """
    Eval√∫a la calidad de un cromosoma usando F1-Score con validaci√≥n cruzada.
    
    Args:
        mask: Vector binario donde 1 = caracter√≠stica seleccionada, 0 = no seleccionada
    
    Returns:
        float: F1-Score promedio obtenido con validaci√≥n cruzada
    """
    
    # Si no se selecciona ninguna caracter√≠stica, penalizar severamente
    if mask.sum() == 0:
        return 0.0
    
    # Seleccionar solo las caracter√≠sticas marcadas con 1
    X_seleccionado = X.loc[:, mask.astype(bool)]
    
    # Configurar validaci√≥n cruzada estratificada
    # Estratificada = mantiene la proporci√≥n de clases en cada fold
    skf = StratifiedKFold(n_splits=KFOLD, shuffle=True, random_state=RANDOM_STATE)
    
    f1_scores = []  # Lista para almacenar F1-Scores de cada fold
    
    # Evaluar en cada fold de la validaci√≥n cruzada
    for train_idx, val_idx in skf.split(X_seleccionado, y):
        # Dividir datos en entrenamiento y validaci√≥n
        X_train, X_val = X_seleccionado.iloc[train_idx], X_seleccionado.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
        
        # Crear y entrenar modelo
        model = LogisticRegression(max_iter=200, random_state=RANDOM_STATE)
        model.fit(X_train, y_train)
        
        # Hacer predicciones y calcular F1-Score
        predictions = model.predict(X_val)
        f1 = f1_score(y_val, predictions, average="macro")  # Macro = promedio no ponderado
        f1_scores.append(f1)
    
    # Retornar el F1-Score promedio de todos los folds
    return float(np.mean(f1_scores))

# ===================================================================
# 4. OPERADORES GEN√âTICOS
# ===================================================================
# Estas funciones implementan los operadores b√°sicos del algoritmo gen√©tico

def crear_individuo_aleatorio():
    """
    Crea un individuo aleatorio (cromosoma) para la poblaci√≥n inicial.
    
    Returns:
        numpy.array: Vector binario representando la selecci√≥n de caracter√≠sticas
    """
    # Crear vector aleatorio con probabilidad 50% para cada caracter√≠stica
    individuo = (np.random.rand(n_features) < 0.5).astype(int)
    
    # Asegurar que al menos una caracter√≠stica est√© seleccionada
    if individuo.sum() == 0:
        # Si ninguna est√° seleccionada, seleccionar una aleatoriamente
        idx_aleatorio = np.random.randint(0, n_features)
        individuo[idx_aleatorio] = 1
    
    return individuo

def seleccion_por_torneo(poblacion, fitnesses, k=TORNEO_K):
    """
    Selecciona un individuo usando torneo de tama√±o k.
    
    Args:
        poblacion: Lista de individuos (cromosomas)
        fitnesses: Lista de valores de fitness correspondientes
        k: Tama√±o del torneo
    
    Returns:
        numpy.array: Individuo seleccionado (copia)
    """
    # Seleccionar k individuos aleatorios de la poblaci√≥n
    indices_competidores = np.random.choice(len(poblacion), size=k, replace=False)
    
    # Encontrar el competidor con mejor fitness
    fitnesses_competidores = [fitnesses[i] for i in indices_competidores]
    ganador_idx = indices_competidores[np.argmax(fitnesses_competidores)]
    
    # Retornar una copia del ganador
    return poblacion[ganador_idx].copy()

def cruce_de_un_punto(padre, madre):
    """
    Realiza cruce de un punto entre dos padres para crear dos hijos.
    
    Args:
        padre: Cromosoma del primer padre
        madre: Cromosoma del segundo padre
    
    Returns:
        tuple: (hijo1, hijo2) - Los dos descendientes
    """
    # Si no se cumple la probabilidad de cruce o hay menos de 2 genes, no cruzar
    if np.random.rand() > PROB_CRUCE or len(padre) < 2:
        return padre.copy(), madre.copy()
    
    # Seleccionar punto de cruce aleatorio (no puede ser 0 ni el final)
    punto_cruce = np.random.randint(1, len(padre))
    
    # Crear hijos intercambiando las partes
    hijo1 = np.concatenate([padre[:punto_cruce], madre[punto_cruce:]])
    hijo2 = np.concatenate([madre[:punto_cruce], padre[punto_cruce:]])
    
    # Asegurar que cada hijo tenga al menos una caracter√≠stica seleccionada
    if hijo1.sum() == 0:
        hijo1[np.random.randint(0, n_features)] = 1
    if hijo2.sum() == 0:
        hijo2[np.random.randint(0, n_features)] = 1
    
    return hijo1, hijo2

def mutacion_bit_flip(individuo):
    """
    Aplica mutaci√≥n bit-flip a un individuo.
    
    Args:
        individuo: Cromosoma a mutar
    
    Returns:
        numpy.array: Cromosoma mutado
    """
    # Crear m√°scara de mutaci√≥n: True donde debe mutar
    mascara_mutacion = np.random.rand(len(individuo)) < PROB_MUTACION
    
    # Crear copia del individuo
    individuo_mutado = individuo.copy()
    
    # Aplicar mutaci√≥n: 1‚Üí0 o 0‚Üí1 donde la m√°scara es True
    individuo_mutado[mascara_mutacion] = 1 - individuo_mutado[mascara_mutacion]
    
    # Asegurar que al menos una caracter√≠stica est√© seleccionada
    if individuo_mutado.sum() == 0:
        individuo_mutado[np.random.randint(0, n_features)] = 1
    
    return individuo_mutado

# ===================================================================
# 5. ALGORITMO GEN√âTICO PRINCIPAL
# ===================================================================
# Esta funci√≥n ejecuta el algoritmo gen√©tico completo

def ejecutar_algoritmo_genetico():
    """
    Ejecuta el algoritmo gen√©tico para selecci√≥n de caracter√≠sticas.
    
    Returns:
        tuple: (mejor_cromosoma, mejor_fitness)
    """
    
    print("\nüß¨ INICIANDO ALGORITMO GEN√âTICO...")
    
    # ===================================================================
    # 5.1 INICIALIZACI√ìN DE LA POBLACI√ìN
    # ===================================================================
    print("üìã Generando poblaci√≥n inicial...")
    
    # Crear poblaci√≥n inicial aleatoria
    poblacion = [crear_individuo_aleatorio() for _ in range(POBLACION_INICIAL)]
    
    # Evaluar fitness de cada individuo en la poblaci√≥n inicial
    print("‚ö° Evaluando fitness de poblaci√≥n inicial...")
    fitnesses = [evaluar_cromosoma(ind) for ind in poblacion]
    
    # Encontrar el mejor individuo inicial
    mejor_individuo = poblacion[int(np.argmax(fitnesses))].copy()
    mejor_fitness = float(np.max(fitnesses))
    
    print(f"üéØ Generaci√≥n 0 | Mejor F1-Score: {mejor_fitness:.4f} | Caracter√≠sticas: {int(mejor_individuo.sum())}")
    
    # ===================================================================
    # 5.2 EVOLUCI√ìN POR GENERACIONES
    # ===================================================================
    for generacion in range(1, GENERACIONES + 1):
        print(f"üîÑ Procesando generaci√≥n {generacion}...")
        
        nueva_poblacion = []
        
        # Crear nueva poblaci√≥n manteniendo el tama√±o original
        while len(nueva_poblacion) < POBLACION_INICIAL:
            # ===================================================================
            # 5.2.1 SELECCI√ìN DE PADRES
            # ===================================================================
            padre1 = seleccion_por_torneo(poblacion, fitnesses)
            padre2 = seleccion_por_torneo(poblacion, fitnesses)
            
            # ===================================================================
            # 5.2.2 CRUCE Y MUTACI√ìN
            # ===================================================================
            hijo1, hijo2 = cruce_de_un_punto(padre1, padre2)
            hijo1 = mutacion_bit_flip(hijo1)
            hijo2 = mutacion_bit_flip(hijo2)
            
            # Agregar hijos a la nueva poblaci√≥n
            nueva_poblacion.extend([hijo1, hijo2])
        
        # Asegurar que la poblaci√≥n tenga exactamente el tama√±o deseado
        poblacion = nueva_poblacion[:POBLACION_INICIAL]
        
        # ===================================================================
        # 5.2.3 EVALUACI√ìN DE LA NUEVA POBLACI√ìN
        # ===================================================================
        fitnesses = [evaluar_cromosoma(ind) for ind in poblacion]
        
        # ===================================================================
        # 5.2.4 ACTUALIZACI√ìN DEL MEJOR INDIVIDUO
        # ===================================================================
        idx_mejor_actual = int(np.argmax(fitnesses))
        if fitnesses[idx_mejor_actual] > mejor_fitness:
            mejor_fitness = float(fitnesses[idx_mejor_actual])
            mejor_individuo = poblacion[idx_mejor_actual].copy()
        
        print(f"üéØ Generaci√≥n {generacion} | Mejor F1-Score: {mejor_fitness:.4f} | Caracter√≠sticas: {int(mejor_individuo.sum())}")
    
    print("‚úÖ Algoritmo gen√©tico completado!")
    return mejor_individuo, mejor_fitness

# ===================================================================
# 6. EJECUCI√ìN DEL ALGORITMO
# ===================================================================
# Ejecutar el algoritmo gen√©tico y obtener resultados

mejor_cromosoma, mejor_score = ejecutar_algoritmo_genetico()

# ===================================================================
# 7. AN√ÅLISIS DE RESULTADOS
# ===================================================================
# Analizar y mostrar los resultados obtenidos

print("\n" + "=" * 60)
print("üìä RESULTADOS DEL ALGORITMO GEN√âTICO")
print("=" * 60)

# Identificar caracter√≠sticas seleccionadas y no seleccionadas
caracteristicas_seleccionadas = [f for f, m in zip(feature_names, mejor_cromosoma) if m == 1]
caracteristicas_no_seleccionadas = [f for f, m in zip(feature_names, mejor_cromosoma) if m == 0]

print(f"üéØ Mejor F1-Score obtenido: {mejor_score:.4f}")
print(f"üìà N√∫mero de caracter√≠sticas seleccionadas: {len(caracteristicas_seleccionadas)}")
print(f"üìâ N√∫mero de caracter√≠sticas descartadas: {len(caracteristicas_no_seleccionadas)}")

print(f"\n‚úÖ CARACTER√çSTICAS SELECCIONADAS ({len(caracteristicas_seleccionadas)}):")
for i, caracteristica in enumerate(caracteristicas_seleccionadas, 1):
    print(f"   {i:2d}. {caracteristica}")

print(f"\n‚ùå CARACTER√çSTICAS DESCARTADAS ({len(caracteristicas_no_seleccionadas)}):")
for i, caracteristica in enumerate(caracteristicas_no_seleccionadas, 1):
    print(f"   {i:2d}. {caracteristica}")

# ===================================================================
# 8. ENTRENAMIENTO DEL MODELO FINAL
# ===================================================================
# Entrenar el modelo final usando solo las caracter√≠sticas seleccionadas

print(f"\nü§ñ ENTRENANDO MODELO FINAL...")

# Seleccionar solo las caracter√≠sticas √≥ptimas
X_optimizado = X[caracteristicas_seleccionadas]

# Crear y entrenar modelo final
modelo_final = LogisticRegression(max_iter=200, random_state=RANDOM_STATE)
modelo_final.fit(X_optimizado, y)

# Hacer predicciones y generar reporte
predicciones_finales = modelo_final.predict(X_optimizado)

print("\n" + "=" * 60)
print("üìã REPORTE DE CLASIFICACI√ìN FINAL")
print("=" * 60)
print(classification_report(y, predicciones_finales, digits=4))

# ===================================================================
# 9. GUARDAR RESULTADOS
# ===================================================================
# Guardar los resultados en archivos para uso posterior

print(f"\nüíæ GUARDANDO RESULTADOS...")

# Guardar lista de caracter√≠sticas seleccionadas
pd.Series(caracteristicas_seleccionadas, name="caracteristica").to_csv(
    "caracteristicas_seleccionadas_ag.csv", index=False
)

# Guardar dataset reducido con solo las caracter√≠sticas seleccionadas
dataset_reducido = pd.concat([X_optimizado, y], axis=1)
dataset_reducido.to_csv("dataset_reducido_ag.csv", index=False)

print("‚úÖ Archivos generados:")
print("   üìÑ caracteristicas_seleccionadas_ag.csv")
print("   üìÑ dataset_reducido_ag.csv")

# ===================================================================
# 10. RESUMEN FINAL
# ===================================================================
print("\n" + "=" * 60)
print("üéâ RESUMEN FINAL")
print("=" * 60)
print(f"üéØ El algoritmo gen√©tico encontr√≥ {len(caracteristicas_seleccionadas)} caracter√≠sticas √≥ptimas")
print(f"üìä F1-Score obtenido: {mejor_score:.4f}")
print(f"üìâ Reducci√≥n de dimensionalidad: {n_features} ‚Üí {len(caracteristicas_seleccionadas)} caracter√≠sticas")
print(f"üí° Porcentaje de caracter√≠sticas mantenidas: {len(caracteristicas_seleccionadas)/n_features*100:.1f}%")
print("=" * 60)

# ===================================================================
# EXPLICACI√ìN DE LA METODOLOG√çA
# ===================================================================
"""
EXPLICACI√ìN DETALLADA DE LA METODOLOG√çA:

1. REPRESENTACI√ìN DEL PROBLEMA:
   - Cada cromosoma es un vector binario de longitud igual al n√∫mero de caracter√≠sticas
   - 1 = caracter√≠stica seleccionada, 0 = caracter√≠stica no seleccionada
   - El objetivo es encontrar la combinaci√≥n que maximiza el F1-Score

2. FUNCI√ìN DE EVALUACI√ìN:
   - Usa validaci√≥n cruzada estratificada para evaluar cada cromosoma
   - Entrena un modelo de Regresi√≥n Log√≠stica con las caracter√≠sticas seleccionadas
   - Calcula F1-Score macro (promedio no ponderado de las clases)
   - Retorna el F1-Score promedio de todos los folds

3. OPERADORES GEN√âTICOS:
   - SELECCI√ìN: Torneo de tama√±o k (selecciona el mejor de k individuos aleatorios)
   - CRUCE: Cruce de un punto (intercambia partes de los cromosomas padre y madre)
   - MUTACI√ìN: Bit-flip (cambia 1‚Üí0 o 0‚Üí1 con probabilidad p_mutaci√≥n)

4. VENTAJAS DE ESTE ENFOQUE:
   - Explora m√∫ltiples combinaciones simult√°neamente
   - No se queda atrapado en √≥ptimos locales
   - Usa validaci√≥n cruzada para evaluaciones robustas
   - Mantiene diversidad gen√©tica a trav√©s de mutaci√≥n

5. APLICACIONES:
   - Reducci√≥n de dimensionalidad en datasets grandes
   - Identificaci√≥n de variables m√°s predictivas
   - Mejora del rendimiento de modelos de ML
   - Reducci√≥n de costos computacionales
"""
